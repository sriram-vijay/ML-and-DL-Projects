{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14cf1c71",
   "metadata": {},
   "source": [
    "# Cat and Dog Image Classification Using CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843fc7ee",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "* In this image classification project, I classified two classes of animals: cats and dogs. I used 8000 images (4000 cats and 4000 dogs) for the training set and 2000 images (1000 cats and 1000 dogs) for the test set. The model was further validated by testing with a single image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48b130e",
   "metadata": {},
   "source": [
    "## Result:\n",
    "* Achieved a training accuracy of 90% and a test accuracy of 80% after 25 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adbd098",
   "metadata": {},
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fff45516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dba91ef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.9.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26048fd",
   "metadata": {},
   "source": [
    "## Part 1 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b4ad7d",
   "metadata": {},
   "source": [
    "### Preprocessing the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72de15cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255,       #Image scaling \n",
    "                                   shear_range = 0.2,      # Feature augmentation\n",
    "                                   zoom_range = 0.2,       # Feature augmentation\n",
    "                                   horizontal_flip = True) # Feature augmentation\n",
    "training_set = train_datagen.flow_from_directory('D:\\ML Projects\\CNN-Project\\Section 40 - Convolutional Neural Networks (CNN)\\dataset/training_set',\n",
    "                                                 target_size = (64, 64), #size reduction to 64*64\n",
    "                                                 batch_size = 32,        #No. of images per batch to train\n",
    "                                                 class_mode = 'binary')  # only two class is present so binary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b96ce56",
   "metadata": {},
   "source": [
    "### Preprocessing the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4d1f2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "test_set = test_datagen.flow_from_directory('D:\\ML Projects\\CNN-Project\\Section 40 - Convolutional Neural Networks (CNN)\\dataset/test_set',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0455059",
   "metadata": {},
   "source": [
    "## Part 2 - Building the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9879521",
   "metadata": {},
   "source": [
    "### Initialising the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bc5f663",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7fc3e2",
   "metadata": {},
   "source": [
    "### Step 1 - Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6537d9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3]))\n",
    "# Filters = Feature detectors =32\n",
    "# Feature detector size=3*3\n",
    "# Input shape is 64*64 and in color so 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8f3dc2",
   "metadata": {},
   "source": [
    "### Step 2 - Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d350e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2)) # pooling grid size =2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e50e12",
   "metadata": {},
   "source": [
    "### Adding a second convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8a2c2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a64933a",
   "metadata": {},
   "source": [
    "### Step 3 - Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "047c1be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2171398f",
   "metadata": {},
   "source": [
    "### Step 4 - Full Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb964d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae05e6ca",
   "metadata": {},
   "source": [
    "### Step 5 - Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74acbc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655c515f",
   "metadata": {},
   "source": [
    "## Part 3 - Training the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5146988",
   "metadata": {},
   "source": [
    "### Compiling the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fb445a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa3926b",
   "metadata": {},
   "source": [
    "### Training the CNN on the Training set and evaluating it on the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09b9344c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "250/250 [==============================] - 206s 820ms/step - loss: 0.6795 - accuracy: 0.5784 - val_loss: 0.6700 - val_accuracy: 0.5590\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 54s 216ms/step - loss: 0.6287 - accuracy: 0.6454 - val_loss: 0.6071 - val_accuracy: 0.6745\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 0.5883 - accuracy: 0.6871 - val_loss: 0.5498 - val_accuracy: 0.7220\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 55s 219ms/step - loss: 0.5537 - accuracy: 0.7132 - val_loss: 0.5335 - val_accuracy: 0.7385\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 64s 256ms/step - loss: 0.5105 - accuracy: 0.7460 - val_loss: 0.5696 - val_accuracy: 0.7215\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 0.5026 - accuracy: 0.7550 - val_loss: 0.5024 - val_accuracy: 0.7590\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 62s 246ms/step - loss: 0.4767 - accuracy: 0.7697 - val_loss: 0.4857 - val_accuracy: 0.7655\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 53s 214ms/step - loss: 0.4617 - accuracy: 0.7793 - val_loss: 0.4804 - val_accuracy: 0.7745\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 61s 243ms/step - loss: 0.4488 - accuracy: 0.7862 - val_loss: 0.4603 - val_accuracy: 0.7775\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 57s 229ms/step - loss: 0.4336 - accuracy: 0.7974 - val_loss: 0.4726 - val_accuracy: 0.7810\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 60s 239ms/step - loss: 0.4101 - accuracy: 0.8094 - val_loss: 0.4735 - val_accuracy: 0.7860\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 55s 219ms/step - loss: 0.4040 - accuracy: 0.8144 - val_loss: 0.4641 - val_accuracy: 0.7865\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 64s 257ms/step - loss: 0.3893 - accuracy: 0.8226 - val_loss: 0.4615 - val_accuracy: 0.7955\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 53s 213ms/step - loss: 0.3799 - accuracy: 0.8314 - val_loss: 0.4612 - val_accuracy: 0.7965\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 62s 249ms/step - loss: 0.3573 - accuracy: 0.8400 - val_loss: 0.4528 - val_accuracy: 0.8035\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 54s 218ms/step - loss: 0.3409 - accuracy: 0.8516 - val_loss: 0.4815 - val_accuracy: 0.7970\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 60s 239ms/step - loss: 0.3286 - accuracy: 0.8568 - val_loss: 0.4722 - val_accuracy: 0.7905\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 57s 230ms/step - loss: 0.3231 - accuracy: 0.8577 - val_loss: 0.4786 - val_accuracy: 0.7860\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 0.3038 - accuracy: 0.8660 - val_loss: 0.5302 - val_accuracy: 0.7975\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 0.2748 - accuracy: 0.8838 - val_loss: 0.5318 - val_accuracy: 0.7960\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 61s 245ms/step - loss: 0.2728 - accuracy: 0.8846 - val_loss: 0.4924 - val_accuracy: 0.8055\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 53s 213ms/step - loss: 0.2580 - accuracy: 0.8854 - val_loss: 0.5098 - val_accuracy: 0.7960\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 63s 252ms/step - loss: 0.2459 - accuracy: 0.8961 - val_loss: 0.5689 - val_accuracy: 0.8115\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 58s 230ms/step - loss: 0.2311 - accuracy: 0.9047 - val_loss: 0.5380 - val_accuracy: 0.7820\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 60s 240ms/step - loss: 0.2252 - accuracy: 0.9075 - val_loss: 0.5494 - val_accuracy: 0.8025\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27b7926d8e0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x = training_set, validation_data = test_set, epochs = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0555b501",
   "metadata": {},
   "source": [
    "## Part 4 - Making a single prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f1d6f681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 20ms/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "test_image = load_img('D:\\ML Projects\\CNN-Project\\Section 40 - Convolutional Neural Networks (CNN)\\dataset/single_prediction/cat_or_dog_6.jpg', target_size = (64, 64))\n",
    "test_image = img_to_array(test_image)       #converting that image to array\n",
    "test_image = np.expand_dims(test_image, axis = 0) #Turning that single image to a batch of 32\n",
    "result = cnn.predict(test_image/255.0)            #Feature scaling\n",
    "training_set.class_indices                        #It helps us to find which value corresponds to which class \n",
    "# Cat - 0, dog - 1\n",
    "\n",
    "if result[0][0] >= 0.5:\n",
    "  prediction = 'dog'\n",
    "else:\n",
    "  prediction = 'cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e38e4c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a390355e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
